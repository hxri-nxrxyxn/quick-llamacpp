services:
  chat:
    image: local-llama-server:latest
    container_name: local-llama-server
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models
    command: >
      -m /models/qwen2.5-3b-instruct-q4_k_m.gguf
      --host 0.0.0.0
      --port 8080
      --chat-template chatml
      -c 4096
      --temp 0.7
      -np 1
      -t 3

  embeddings:
    image: local-llama-server:latest
    container_name: local-llama-embeddings
    restart: unless-stopped
    ports:
      - "8081:8080"
    volumes:
      - ./models:/models
    command: >
      -m /models/all-MiniLM-L6-v2-f16.gguf
      --host 0.0.0.0
      --port 8080
      --embedding